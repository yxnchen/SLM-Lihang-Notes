# 提升方法

[TOC]



> 在分类问题中，提升方法通过改变训练样本的权重，学习出多个分类器，并将这些分类器进行线性组合，提高分类的性能

## 8.1 提升方法AdaBoost算法

### 8.1.1 提升方法的基本思路

- **思路**：对于一个复杂任务，将多个专家的判断进行适当的综合所得的判断要比其中任何一个专家单独的判断好；
- **强可学习**（strongly learnable）：在概率近似正确（probably approximately correct，PAC）学习的框架中，一个概率（一个类）如果存在一个多项式的学习算法能够学习它，并且正确率很高，称这个概念是强可学习的；
- **弱可学习**（weakly learnable）：一个概念如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么称这个概念是弱可学习的；

- 已证明：<u>*强可学习与弱可学习是等价的*</u>，在PAC框架下，一个概念是强可学习的充要条件是它是弱可学习的；
- 对于分类问题，给定一个训练集，求比较粗糙的分类规则（弱分类器）比求精确的分类规则（强分类器）容易，提升方法反复学习得到一系列弱分类器，然后组合成一个强分类器；
- 大多数方法是改变训练数据的概率分布（训练数据的权值分布），然后调用弱学习算法；



- 两个问题：1）每一轮如何改变训练数据的权值或概率分布；2）如何把弱分类器组合成强分类器；
- AdaBoost：1）提高那些被前一轮弱分类器错误分类样本的权值，而降低正确分类的权值；2）采用加权多数表决方法，即加大分类误差率小的弱分类器的权值，使其在表决中发挥较大作用；

### 8.1.2 AdaBoost算法

- 假设给定一个二分类的训练数据集$T$；

- **AdaBoost算法**：

  1. **输入**训练数据集$T$，弱学习算法；

  2. 初始化训练数据的权值分布：
     $$
     D_1=(w_{11},\dots,w_{1i},\dots,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,\dots,N
     $$

  3. 对于$m=1,2,\dots,M$：

     1. 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器：
        $$
        G_m(x):\mathcal{X}\Rightarrow\{-1,+1\}
        $$

     2. 计算$G_m(x)$在训练数据上的分类误差率：
        $$
        e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\neq y_i)
        $$

     3. 计算$G_m(x)$的系数，这里对数是自然对数：
        $$
        \alpha_m=\frac{1}{2}\log \frac{1-e_m}{e_m}
        $$

     4. 更新训练数据集的权值分布：
        $$
        D_{m+1}=(w_{m+1,1},\dots,w_{m+1,i},\dots,w_{m+1,N}),\\ w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)), i=1,2,\dots,N
        $$
        其中$Z_m=\sum_{i=1}^{N}w_{mi}\exp(-\alpha_my_iG_m(x_i))$是归一化因子，使得$D_{m+1}$是一个概率分布；

  4. 构建基本分类器的线性组合：
     $$
     f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
     $$
     得到最终的分类器：
     $$
     G(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{m=1}^{M}\alpha_mG_m(x)\right)
     $$



- 算法解释：

  1. 初始化时假设具有均匀的权值分布，保证了在第一步中能够在<u>*原始数据上学习*</u>基本分类器；

  2. 计算基本分类器$G_m(x)​$分类误差率时，$e_m=P(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{mi}​$实质上是被$G_m(x)​$<u>*误分类样本的权值之和*</u>，因此权值分布与分类器误差率有关系；

  3. 基本分类器的系数$\alpha_m$表示$G_m(x)$在最终分类器中的重要性：当$e_m\leq\frac{1}{2}$时，$\alpha_m\geq0$，且$\alpha_m$随着$e_m$的减少而增大，也即分类误差率越小的基本分类器在最终分类器的作用更大；

  4. 更新训练集的权值分布时，可以写成
     $$
     w_{m+1,i}=\left\{
     \begin{array}{l@{\quad}l@{\quad}}
     \frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i)=y_i\\
     \frac{w_{mi}}{Z_m}e^{\alpha_m}, & G_m(x_i)\neq y_i\\
     \end{array}
     \right.
     $$

     也就是说被基本分类器误分类的样本的权值得意扩大，而正确分类的却被缩小，相比较误分类样本的权值被放大$e^{2\alpha_m}=\frac{e_m}{1-e_m}$倍；

  5. 线性组合$f(x)$实现了$M$个基本分类器的加权表决，系数表示了各个基本分类器的重要性，但是所有$\alpha_m$之和不等于1，$f(x)$的符号决定了实例$x$的类，$f(x)$的绝对值表示分类的确信度；

## 8.2 AdaBoost算法的训练误差分析

- AdaBoost最基本的性质是能够在学习过程中不断减少训练误差，即在训练数据上的分类误差率；
- **AdaBoost的训练误差界**（定理）：AdaBoost算法最终分类器的训练误差界为：

$$
\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i)\leq\frac{1}{N}\sum_{i}\exp(-y_if(x_i))=\prod_{m}Z_m
$$

​	此定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使得训练误差下降得最快；

- **二分类问题AdaBoost的训练误差界**（定理）：

$$
\prod_{m=1}^{M}Z_m=\prod_{m=1}^{M}[2\sqrt{e_m(1-e_m)}]=\prod_{m=1}^{M}\sqrt{(1-4\gamma_m^2)}\leq\exp\left(-2\sum_{m=1}^{M}\gamma_m^2\right)
$$

​	其中$\gamma_m=\frac{1}{2}-e_m$；

- **推论**：如果存在$\gamma>0$，对所有的$m$有$\gamma_m\geq\gamma$，则

$$
\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i)\leq\exp(-2M\gamma^2)
$$

​	则表明在此条件下，AdaBoost的训练误差是<u>*以指数速率下降*</u>的；

- AdaBoost不需要知道下界$\gamma$，其具有适应性，能够适应弱分类器各自的训练误差率，因此叫Adaptive Boosting；

## 8.3 AdaBoost算法的解释

